#RegionServer Sizing Rules of Thumb
ars Hofhansl wrote a great blog post about RegionServer memory sizing. 结果是你可能需要比你认为的更多的内存。它受到region size、memstoresize、Hdfs replication factor，以及其他要检查的事项的影响。

>  Personally I would place the maximum disk space per machine that can be served exclusively with HBase around 6T, unless you have a very read-heavy workload. In that case the Java heap should be 32GB (20G regions, 128M memstores, the rest defaults).

>   — Lars Hofhansl

>   http://hadoop-hbase.blogspot.com/2013/01/hbase-region-server-memory-sizing.html

##35. 根据列族数量
当前Hbase在处理2或3个以上列族时，表现不是很好，所以控制你schema中的列族数尽量少点。目前flush和compaction是在每个region的基础上进行。所以当一个列族操作大量数据时会引发flush，而相连的列族也会进行flush，即使它们操作的数据量很少。当许多列族存在flush和compaction相互作用时，会产生大量不必要的i/o（这个通过使flush和compaction只针对一个列族来解决）

尽量在你的schema中使用一个列族。只有数据访问固定在特定的列时，可以引入第二个和第三个列族.例如，你有两个列族,但你查询的时候总是访问其中的一个，从来不会两个一起访问。

###35.1. 列族的基数
当表中存在多个列在时，注意表的基数（行的数量）。如果ColumnfamliyA有100万行，而ColumnFamilyB有10亿行，CFA的数据可能分散在许多regions中（及RS中），这将导致CFA的scans操作非常低效。

##36. 行键设计
###36.1. Hotspotting
Hbase中行是按行键的字典序列排序的。这种设计优化scans操作，让你可以将相关的行存储在一起，或靠近的行将一起被读取。当然，row keys设计不好是hotspotting一个常见的原由。hotspotting发生在当大量客户流量流向一个节点或一些节点或一个集群时。这个流量可代表读、写或其他操作。它可能会淹没负责管理这个region的单台机器，导致其性能下降及导致region不可用。也可能会对同一region server管理的其他region产生不好的影响，如不能服务所需的负载。要想集群被充分使用，设计数据访问模式很重要。

避免写操作时的hotspotting，把row key设计成这样：行确实是需要在同一个region。但in the bigger picture，数据会被写倍集群中多个region，而不是一次一个。下面会描述一些常见的避免hotspotting技术，以及它们的优缺点。

_**Salting**_

这里的salting与加密无关，指的是在row key的开始中加入随机数据。这里，salting指的是给row key随机配置一个前缀，使它与原本排序不同。可能的前缀数量要与你想要散布数据的region个数相同。如果你有一些“hot”row key模式反复出现在其他更均匀行中，salting会有帮助的。思考下面的例子，Salting可以传播写入到多个负载，以及证明一些读的负面启示。

_**Example 16. Salting Example**_

>  假定你有以下行键表，你的表被分割成一个字母表中的字母一个区域，前缀‘a’是一个区域，‘b’是另一个。这个表中，同一个region中所有的行以f开头。这个例子关注下面这样的行键：

>      foo0001
>      foo0002
>      foo0003
>      foo0004

>   现在假定你你将这些分布在四个不同的区域，使用四个不同的salts：a，b，c，d。这个场景中，每个单词前缀都在不同区域上。在使用salts后，你有下面这样的行键。既然你可以写到四个分开的区域，理论上，当写入时，你有四倍的吞吐量比吸入同一region。

>      a-foo0003
>      b-foo0001
>      c-foo0004
>      d-foo0002

>    Then, if you add another row, it will randomly be assigned one of the four possible salt values and end up near one of the existing rows.

>      a-foo0003
>      b-foo0001
>      c-foo0003
>      c-foo0004
>      d-foo0002

>    既然这个分配是随机的，如果你想要获得字典序列的行，你要做更多工作。这样，salting增加来写入时的吞吐量，但读时会有消耗。

_**Hashing**_

不使用随机分配，你可以使用一个单向散列的方式，可以使已知行总是被同一前缀salted，这种方式是一种在RS上分散负载，而允许读取时可预测的方式。使用确定的hash，允许客户端重建完整的行键以及像平常一样使用Get操作获取完整的行。

_**Example 17. Hashing Example**_

>  Given the same situation in the salting example above, you could instead apply a one-way hash that would cause the row with key foo0003 to always, and predictably, receive the a prefix. Then, to retrieve that row, you would already know the key. You could also optimize things so that certain pairs of keys were always in the same region, for instance.

_**Reversing the Key**_

第三个常见的避免hotspotting的技巧是颠倒固定宽度或数量的行键，这样最频繁改变的部分是第一个。这有效地随机化行键，但牺牲了行排序属性。

###36.2 单调递增的行键/时间序列数据
在Tom White的Hadoop: The Definitive Guide一书中，有一个章节描述了一个值得注意的问题：在一个集群中，一个导入数据的进程一动不动，所有的client都在等待一个region(就是一个节点)，过了一会后，变成了下一个region...如果使用了单调递增或者时序的key就会造成这样的问题。详情可以参见IKai画的漫画monotonically increasing values are bad。使用了顺序的key会将本没有顺序的数据变得有顺序，把负载压在一台机器上。所以要尽量避免时间戳或者(e.g. 1, 2, 3)这样的key。

如果你确实需要上传时间序列数据到Hbase中，可以参考下OpenTSDB，这是个成功的例子。它有一页描述它在Hbase中使用的schema。OpenTSDB中使用了有效的[metric_type][event_timestamp]键格式，这个乍一看会与之前不要在键中使用时间戳的建议相悖。然而，这里的不同之处是没有把时间戳放在开头，这种设计的假设是会有许多不同的metric types，因此即使不断有metric type混合的输入数据流，Puts会被分散到表中各个region上。

###36.3. 试着最小化行列尺寸
HBase中，值总是承载着它们的坐标；当cell值在系统中传递时，它总是和它的列、行和时间戳一起。如果你的行和列的名字要是太大(甚至比value的大小还要大)的话，你可能会遇到一些有趣的情况。
